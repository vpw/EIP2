## EIP2 - Session 2 - Assignment 2A ##

#### Vardhan Walavalkar, Batch 5 ####

**Exercise**

Reimplement the tables from https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/ by using different random intitialization values.



**Step 0**  Read input and output


| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   |      |      |      |      |      |      |                    |      |      |                          |      |      |      |      |        | 1 ||
| 1    | 0    | 1    | 0   |      |      |      |      |      |      |                    |      |      |                          |      |      |      |      |        | 1 ||
| 0    | 1    | 0    | 1    |      |      |      |      |      |      |                    |      |      |                          |      |      |      |      |        | 0 ||
|      |      |      |      |      |      |      |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||


</table>

**Step 1** Initialize weights and biases with random values (There are methods to 
initialize weights and biases but for now initialize with random values)

| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 |                    |      |      |                          |      |      | 0.38 | 0.71 |        | 1 ||
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.13 |      |      |      |                    |      |      |                          |      |      | 0.11 |      |        | 1 ||
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      |                    |      |      |                          |      |      | 0.77 |      |        | 0 ||
|      |      |      |      | 0.44 | 0.15 | 0.36 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||

**Step 2:** Calculate hidden layer input:
hidden_layer_input= matrix_dot_product(X,wh) + bh


| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 |                          |      |      | 0.38 | 0.71 |        | 1 ||
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.13 |      |      |      | 0.69 | 1.47 | 1.23 |                          |      |      | 0.11 |      |        | 1 ||
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 |                          |      |      | 0.77 |      |        | 0 ||
|      |      |      |      | 0.44 | 0.15 | 0.36 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||

**Step 3:** Perform non-linear transformation on hidden linear input
*hiddenlayer_activations = sigmoid(hidden_layer_input)*

| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 | 0.76 | 0.84 | 0.83 | 0.38 | 0.71 |        | 1 ||
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.13 |      |      |      | 0.69 | 1.47 | 1.23 | 0.67 | 0.81 | 0.77 | 0.11 |      |        | 1 ||
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 | 0.79 | 0.86 | 0.64 | 0.77 |      |        | 0 ||
|      |      |      |      | 0.44 | 0.15 | 0.36 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||

**Step 4:** Perform linear and non-linear transformation of hidden layer activation at output layer

output_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout
 *output = sigmoid(output_layer_input)*


| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 | 0.76 | 0.84 | 0.83 | 0.38 | 0.71 | 0.85 | 1 ||
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.13 |      |      |      | 0.69 | 1.47 | 1.23 | 0.67 | 0.81 | 0.77 | 0.11 |      | 0.84 | 1 ||
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 | 0.79 | 0.86 | 0.64 | 0.77 |      | 0.83 | 0 ||
|      |      |      |      | 0.44 | 0.15 | 0.36 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||

**Step 5:** Calculate gradient of Error(E) at output layer
*E = y-output*


| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 | 0.76 | 0.84 | 0.83 | 0.38 | 0.71 | 0.85 | 1 |0.15|
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.13 |      |      |      | 0.69 | 1.47 | 1.23 | 0.67 | 0.81 | 0.77 | 0.11 |      | 0.84 | 1 |0.16|
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 | 0.79 | 0.86 | 0.64 | 0.77 |      | 0.83 | 0 |-0.83|
|      |      |      |      | 0.44 | 0.15 | 0.36 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||

**Step 6:** Compute slope at output and hidden layer
*Slope_output_layer= derivatives_sigmoid(output)*
*Slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)*

| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 | 0.76 | 0.84 | 0.83 | 0.38 | 0.71 | 0.85 | 1 |0.15|
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.13 |      |      |      | 0.69 | 1.47 | 1.23 | 0.67 | 0.81 | 0.77 | 0.11 |      | 0.84 | 1 |0.16|
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 | 0.79 | 0.86 | 0.64 | 0.77 |      | 0.83 | 0 |-0.83|
|      |      |      |      | 0.44 | 0.15 | 0.36 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||

**Step 7:** Compute delta at output layer

*d_output = E * *slope_output_layer* * *lr*


| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 | 0.76 | 0.84 | 0.83 | 0.38 | 0.71 | 0.85 | 1 |0.15|
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.13 |      |      |      | 0.69 | 1.47 | 1.23 | 0.67 | 0.81 | 0.77 | 0.11 |      | 0.84 | 1 |0.16|
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 | 0.79 | 0.86 | 0.64 | 0.77 |      | 0.83 | 0 |-0.83|
|      |      |      |      | 0.44 | 0.15 | 0.36 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||
|||||||||||**Slope hidden layer**||||||||**Slope output**||**E**|
|||||||||||0.19|0.14|0.14||||||0.13||0.15|
|||||||||||0.22|0.15|0.18||||||0.14||0.16|
|||||||||||0.17|0.12|0.23||||||0.14||-0.83|
||||||||||||||||||||**delta output**||
||||||||||||||||||||0.02||
||||||||||||||||||||0.02||
||||||||||||||||||||-0.12||



**Step 8:** Calculate Error at hidden layer

Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)


| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 | 0.76 | 0.84 | 0.83 | 0.38 | 0.71 | 0.85 | 1 |0.15|
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.13 |      |      |      | 0.69 | 1.47 | 1.23 | 0.67 | 0.81 | 0.77 | 0.11 |      | 0.84 | 1 |0.16|
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 | 0.79 | 0.86 | 0.64 | 0.77 |      | 0.83 | 0 |-0.83|
|      |      |      |      | 0.44 | 0.15 | 0.36 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||
|||||||||||**Slope hidden layer**|||**error at hidden layer**|||||**Slope output**||**E**|
|||||||||||0.19|0.14|0.14|0.007|0.002|0.015|||0.13||0.15|
|||||||||||0.22|0.15|0.18|0.008|0.002|0.017|||0.14||0.16|
|||||||||||0.17|0.12|0.23|-0.044|-0.013|-0.089|||0.14||-0.83|
||||||||||||||||||||**delta output**||
||||||||||||||||||||0.02||
||||||||||||||||||||0.02||
||||||||||||||||||||-0.12||

**Step 9:** Compute delta at hidden layer

*d_hiddenlayer = Error_at_hidden_layer \* slope_hidden_layer*


| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 | 0.76 | 0.84 | 0.83 | 0.38 | 0.71 | 0.85 | 1 |0.15|
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.13 |      |      |      | 0.69 | 1.47 | 1.23 | 0.67 | 0.81 | 0.77 | 0.11 |      | 0.84 | 1 |0.16|
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 | 0.79 | 0.86 | 0.64 | 0.77 |      | 0.83 | 0 |-0.83|
|      |      |      |      | 0.44 | 0.15 | 0.36 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||
|||||||||||**Slope hidden layer**|||**error at hidden layer**|||||**Slope output**||**E**|
|||||||||||0.19|0.14|0.14|0.007|0.002|0.015|||0.13||0.15|
|||||||||||0.22|0.15|0.18|0.008|0.002|0.017|||0.14||0.16|
|||||||||||0.17|0.12|0.23|-0.044|-0.013|-0.089|||0.14||-0.83|
|||||||||||**delta hidden layer**|||||||||**delta output**||
|||||||||||0.001|0.0|0.002|||||||0.02||
|||||||||||0.002|0.0|0.003|||||||0.02||
|||||||||||-0.007|-0.002|-0.021|||||||-0.12||

**Step 10:** Update weight at both output and hidden layer

wout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)*learning_rate
 wh =  wh+ matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_rate


| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 | 0.76 | 0.84 | 0.83 | 0.37 | 0.71 | 0.85 | 1 |0.15|
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.12 |      |      |      | 0.69 | 1.47 | 1.23 | 0.67 | 0.81 | 0.77 | 0.10 |      | 0.84 | 1 |0.16|
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 | 0.79 | 0.86 | 0.64 | 0.77 |      | 0.83 | 0 |-0.83|
|      |      |      |      | 0.43 | 0.15 | 0.35 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||
|||||||||||**Slope hidden layer**|||**error at hidden layer**|||||**Slope output**||**E**|
|||||||||||0.19|0.14|0.14|0.007|0.002|0.015|||0.13||0.15|
|||||||||||0.22|0.15|0.18|0.008|0.002|0.017|||0.14||0.16|
||**Learning rate**|0.1||||||||0.17|0.12|0.23|-0.044|-0.013|-0.089|||0.14||-0.83|
|||||||||||**delta hidden layer**|||||||||**delta output**||
|||||||||||0.001|0.0|0.002|||||||0.02||
|||||||||||0.002|0.0|0.003|||||||0.02||
|||||||||||-0.007|-0.002|-0.021|||||||-0.12||

**Step 11:** Update biases at both output and hidden layer

*bh = bh + sum(d_hiddenlayer, axis=0) \* learning_rate* *bout = bout + sum(d_output, axis=0)\*learning_rate*


| X      |         |        |      | Wh   |      |      | bh   |      |      | hidden_layer_input |      |      | hidden_layer_activations |  |  | wout | bout | output | y    | E    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------------------ | ---- | ---- | ------------------------ | ---- | ---- | ------ | ---- | ---- | ---- | ---- |
| 1    | 0    | 1    | 1   | 0.31 | 0.29 | 0.55 | 0.04 | 0.71 | 0.10 | 1.13 | 1.62 | 1.59 | 0.76 | 0.84 | 0.83 | 0.37 | 0.71 | 0.85 | 1 |0.15|
| 1    | 0    | 1    | 0   | 0.84 | 0.92 | 0.12 |      |      |      | 0.69 | 1.47 | 1.23 | 0.67 | 0.81 | 0.77 | 0.10 |      | 0.84 | 1 |0.16|
| 0    | 1    | 0    | 1    | 0.35 | 0.46 | 0.58 |      |      |      | 1.31 | 1.79 | 0.58 | 0.79 | 0.86 | 0.64 | 0.77 |      | 0.83 | 0 |-0.83|
|      |      |      |      | 0.43 | 0.15 | 0.35 |      |      |      |                    |      |      |                          |      |      |      |      |        |      ||
|||||||||||**Slope hidden layer**|||**error at hidden layer**|||||**Slope output**||**E**|
|||||||||||0.19|0.14|0.14|0.007|0.002|0.015|||0.13||0.15|
|||||||||||0.22|0.15|0.18|0.008|0.002|0.017|||0.14||0.16|
||**Learning rate**|0.1||||||||0.17|0.12|0.23|-0.044|-0.013|-0.089|||0.14||-0.83|
|||||||||||**delta hidden layer**|||||||||**delta output**||
|||||||||||0.001|0.0|0.002|||||||0.02||
|||||||||||0.002|0.0|0.003|||||||0.02||
|||||||||||-0.007|-0.002|-0.021|||||||-0.12||


